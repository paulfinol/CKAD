etcd: data base, key/value store
scheduler: decides where to place the containers
controller: make decision to create new containers when necessary
API server: front end for K8s,  external resources connect to cluster through it
worker nodes/minios: virtual or physical server where the PODs/container are
controller node: where all the service like controller, scheduler, DB, API server
cluster: a group of worker nodes
kubelet: agent that runs on each node of the cluster
kubectl: command line utility, to deploy and manage application, admin cluster etc
container runtime: what powers the container, for example docker
PODs smallest object that can be created in kubernetes
one POD one container application at least of the same type,  there could be additional helper containers inside the same POD
they communicate each other because they share the same namespace
images are downloaded from dockerhub by default

minikube: a way to have a local K8s cluster 

kubectl run pod-name --image = image_name
	get pods: shows all pods
	describe pod xxx: more detailed 
	get pods -o wide : shows all pods in a WIDE list with more detail

YAML Pods
under metadata can only be specified name and labels, for labels any kind
each object has specified format.
for PODS
	api version
	kind
	metadata
	spec
kubectl apply -f file.yaml
config for kubernetes in YML extension VS code 
    "yaml.schemas": {
        "kubernetes": "*.yaml"
    }

Replication controller
Handles the pods and recreates it in case is down
replica set is replacing the replication controller

replicaset is defined with apiversion: apps/v1
		 		kind: ReplicaSet
				metadata
					name
					lables
				spec
					template: /* used for the replica set to create POD in case it dies
					/*
					Copy all POD definition
					*/
				replicas: how many PODs
				selector:
ReplicaSet can also handle PODs that were not created together with the replicaSet object.
For that, specify selector
labels and selectors works as identifier for PODs because IPs are ephemeral
kubectl get replicaset
	delete replicaset name
	replace -f file.yaml
	 scale replicaset myapp-replicaset --replicas=2 /* scale without having to modify the file
	describe
If a POD created under tthe selector of the replica set, first checks if complies with the replicas #
kubectl edit replicaset name - edits the configuration in the cluster directly

kubernetes deployments: has the capabilities to enforce several types of upgrades
blue/gree
rolling updates: updates pods one by one
the deployment automatically creates a replica set
a new object deployment is created, thats the only difference from replica set

Updates and rollbacks

a deployment triggers a Rollout a new rollout creates a new deployment revision
the revision helps to keep track of the deployments 
kubectl rollout status deployment_name
		history deployment_name
deployment strategy
Recreate: destroy everything bring everything up with new version
rolling update (default): updates pods one by one
with describe can see if one or the other was applied

for the upgrades
the rollout can be triggered modifing the configuration file and then kubectl apply -f 
or modifying paramenters on the CLI: kubectl set image deployment/myapp-deployment \ nginx=nginx:1.9.1 ---- the cofniguration file will have a diferent info

the rollout is actually done creating a new replicaSet and creating new pods as the old replicaset is deleting them

Commands


kubectl create -f deployment.yml (--record to record the cause of change)
	get deployment
	apply -f deployment.yml
	set image deployment/myapp-deployment \ nginx=nginx:1.9.1
	rollout status deployment.apps/deployment_name
	rollout history deployment.apps/deployment_name
	kubectl rollout undo deployment.apps/deployment_name



kubectl rollout status deployment.v1.apps/prometheus-grafana


Kubectl run para PODS
	create -f filename command is imperative.
	apply -f finalme declarative syntax
	edit edit configuration file (solo ciertos parametros de un POD puedes ser editados)
	scale

Networking
a node created an internal private network from where IPs are assigned to all PODs reaching communication between all PODs
However IPs could change
for multiple nodes each node has its internal private newtwork if they have the same CIRD could cause conflict
K8s doesnt provide networking but instead defines the requirements for it to work
	All containers/POS can communicate to one another without NAT
	all nodes can communicate with all containers and vice-versa without NAT
There are networking solutions that take care of routing, like Flannel, VMware, CISCO etc

Services
Object that Enable connectivity between PODs, internal, external
	NodePort: for external comunication, listen to a port on the node and forwards that requests to a port on POD running the application 
		Port: port on the service, the only mandatory
		NodePort: port on the node (30000-32767), if not specified any in the range is assigned
		TargetPort: port on the POD, if not assigned then is the same as Port
		CLuster IP: a service is like a virtual server inside the node that gets assigned an IP
	if a selector matches PODs in multiple nodes K8s creates a service that covers all the nodes that hold the PODs
	the algoritm to automatically share packages is Random and session affinity is allowed
	ClusterIP:
		the services creates a virtual IP inside the cluster to enable internal comunication
		the cluster IP is assigned to a set of PODs that make a tier of an application for example
	Load Balancer:
	K8s is compatible with load balancing solution from cloud operators
Services are creater with a YAML like all objects
	Kind: Service
	spec
		Type: is CLuserIP by default, NodePort or LoadBalancer
		port:
		- TargetPort
		  Port
		  nodePort
		selector: (all the labels matching the PODs)
			app: my-app
			tier: frontend
Kubectl create -f service.yaml
kubectl expose pod podname --port=## --name service-name (connects pods with service)
kubectl ceate service servicetype servicename --tcp=6379:6379 (<port>:<targetPort>/ cannot pass selectors, instead will assume selector ass app=servicename) 
		



NAMESPACES
an object for logical isolation inside a cluster
 able to reserver quota Resource limits within a namespace
 able to define policies for each namespace
when a service is created a DNS entry is added automatically in the format
	db-service.dev.svc.cluster.local	service Name.namespace.svc.default domain of the K8s cluster

Create a name space file and the kubectl create -f
apiVersion: v1
kind: Namespace
metadata:
	name: name
OR
kubectl create namespace name

kubectl get pods --namespace=name or in all namespaces kubectl get pods --all-namespace
kubectl create -f pod.yaml --namespace=name OR 
inside the configuration file
metadata:
	namespace : name

default the namespace
kubectl config set-context $(kubectl config current-context) --namespace=name

to limit resources define a resource quota and then kubectl create -f
apiVersion: v1
kind: ResourceQuota
metadata:
	name: quota-name
	namespace: name

Docker commands

A container only lives as long as the process inside it is alive, is not meant to live forever
CMD command that will be ran when the container starts, if a new command is defined in command line to change behavior then it will replace CMD
CMD command param1
CMD sleep 5
docker run docker-image 
ENTRYPOINT will be ran using the argument specified in command line
ENTRYPOINT command
docker run docker-image arg
If command line is ran without an argument for ENTRYPOINT the a default value could be specified in CMD
ENTRYPOINT command
CMD param1
if need to overwrite the ENTRYPOINT the can be done in the CLI
docker run --entrypoint command docker-image argument
if there is a docker file command override in the pod definition under spec/containers would be. HAS TO BE DEFINED IN JSON FORMAT
ENTRYPOINT ["sleep"] ------- command: ["sleep2.0"]
CMD["5"]	     ------- args: ["10"]

Environment Variables
docker run -e APP_COLOR=pink docker_image
defined in spec/ containers
env:
   - name: APP_COLOR	Plain Value
     value: pink
   - name: APP_COLOR	configMap
     valueFrom: 
        configMapKeyRef:
   - name: APP_COLOR	SecretKey
     valueFrom: 
        SecretKeyRef:	

Config maps 
are used to pass configuration data in the form of key value pairs so we can take this information out of the pod and manage it in centralize way
like any object can be created imperative or declarative
kubecl create configmap --from-literal=key=value --from-literal=key1=value2
also from a file
kubecl create configmap --from-file=path to file
declarative
apiVersion: v1
kind ConfigMap
metadata
	name
data
	key: Vale
	key2: value2
when the file is ready then kubectl create -f config.yaml
create the config man for each pod name properly
to config the POD with the configmap
envFrom:
- configMapRef: 
    name: config-map
or a single variable 
env:
   - name: APP_COLOR	
     valueFrom: 
        configMapKeyRef: 
	   name: config-map
	   key: APP_color
or a volume
volumes:
- name: config_map_volume
  configMap: 
     name: config-map

secrets
Kubectl create secret generic secrte_name --from-literal=key=value
					  --from-file=path to file
kubectl create -f sercret.yaml
apiversion: v1
kind secret
metadata:
	name: app-seret
data:
	DB_host: srg5&*
	DB_user: sdfd#5%
	DB_password: DFwe@#F
information inside the secret must be encoded, anyone can decode it so is really not safe
In linux using base64 to encode
	echo -n 'secret' | base64
and decode
	echo -n 'secret' | base64 --decode
to inlcude in the pod
envFrom:
- secretRef: 
    name: config-map
or a single variable 
env:
   - name: DB_Password
     valueFrom: 
        secretKeyRef: 
	   name: app-secret
	   key: DB_password
or a volume
volumes:
- name: app_secret_volume
  secret:
     secretName: app-secret

if the secret is mounted as a volume in the POD each attribute/value in the secret is created as file 

Docker Security

containers are isolated using namespaces in linux, each container has its own namespace
the container can see only what is inside its namespace
the host can see can see the process in the containers but with differente process IDs

users in context of security
the docker host has a set of users,
by default docker runs processed within the container as root user, to enforce a different user can be done in the docker run command --user=userid
or in the dockerfile USER userid, then the image will be ran always wit the user ID
the user within the container is limited to some privileges only by defautl to modify this behavior in the docker run user --cap-add/--cap-drop to add or remove or --priviledge for all priviledges enabled

Kubernetes Security

setting could be configured at POD (applies to all containers inside the POD) level or container level. container has higher priority
POD level
spec
	containers
	securityContext:
		runAsUser: ###
Container level
spec
	containers
		- name: ubuntu
		  image: ubuntu
		  securityContext:
		    runAsUser: ###
		    capabilities: (only supported at container level)
		      add:
		      delete:

Service Accounts
user account 
service account
kubectl create serviceaccount dashboard-sa
when a service account is created first creates a secret to save a token and the secret is linked to service account
the token is used for authentication
if the service account is for a third party the process needs to be done manually
if the service account is for internal usage the secret could be attached to a volume and then the service could reference that volume to find the token
the process can be simpler by automatically mounting the secret as volume inside the POD that holds the application 
it happens automatically when the namespace is created, there is default service account for each namespace when a POD is created the default service account and its token are automatically mounted in a volume inside the POD
to specify a different service account
spec
	container
	serviceAccountName: service-account
k8s automatically mounts the default service account in a volume to override this behavior in the token YAML set automountServiceAccountToken:False

resource requirement
minimum amount required to run
if thre are not enough resources on the node the scheduler will hold the POD deployment
it can be modified on the YAML under the container specifications
spec
  container
  - name
    resources:
      request:
        memory: "1Gi"
	cpu: 1 (count of vCPU like vCPU in Azure)
1 G = 1,000,000,000 bytes
1 Gi= 1,073,741,824 bytes
same with other units

limit
maximum consumption
      limits:
        memory: "1Gi"
	cpu: 1 (count of vCPU like vCPU in Azure)
if a pod tries to consume more CPU than limit, is not allowed
is allowed to consume more memory, but it does it constantly then is terminated

limitrange
by default the K8s can limit the resoirces a POD or a container within a POD reqires by setting a limit range to that namespace
apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container
kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults.yaml --namespace=default-mem-example

Taints and Tolerations
Node can be tainted to allow only certain PODs (tolerants PODs), maybe Dedicated Nodes or Nodes with Special Hardware
Master node is tainted by default there are no tolerants PODs
there are 3 actions for PODs facing tainted nodes
	if a taint with effect NoExecute is added to a node, then any pods that do not tolerate the taint will be evicted immediately, and pods that do tolerate the taint will never be evicted
	 However, a toleration with NoExecute effect can specify an optional tolerationSeconds
	effect NoSchedule then Kubernetes will not schedule the pod onto that node
	effect PreferNoSchedule then Kubernetes will try to not schedule the pod onto the node

kubectl taint nodes node1 key1=value1:NoSchedule/PreferNoSchedule/NoExecute 

A toleration "matches" a taint if the keys are the same and the effects are the same, and:
the operator is Exists (in which case no value should be specified), or the operator is Equal and the values are equal.
Empty means wildcard
	An empty key with operator Exists matches all keys, values and effects which means this will tolerate everything.
	An empty effect matches all effects with key key1.

spec
 containers:
 toleration:
 - key: "key1"
   operator: "Equal"
   value: "value1"
   effect: "NoSchedule"

Node Selector
Select the nodes where PODs should run, only for limited POD placement
spec
	containers:
	nodeSelector:
		size: Large

in the node:  kubectl label nodes node-name size=large

Node affinity Schedule POD a on target node
POD affinity schedule POD a near POD b
POD anti-affinity DO NOT schedule POD a near POD b

requiredDuringSchedulingIgnoredDuringExecution: The scheduler can't schedule the Pod unless the rule is met. This functions like nodeSelector, but with a more expressive syntax.
preferredDuringSchedulingIgnoredDuringExecution: The scheduler tries to find a node that meets the rule. If a matching node is not available, the scheduler still schedules the Pod.
 IgnoredDuringExecution means that if the node labels change after Kubernetes schedules the Pod, the Pod continues to run.
requiredDuringSchedulingREQUIREDDuringExecution: a POD is evicted from the node in case it doesnt match the selector

taint and tolerations can b used TOGETHER with node affinity